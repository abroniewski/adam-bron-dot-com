Focus mostly on how to make things repeatable and reproducible

# Fundamentals
### Empirisicm:
## Research Questions
Need to be specific and have good measures by which to confidently answer the question in the end
### Methods 
**Exploratory**: descriptive (how is it different), classification (what are the properties, how is it measured), existence (does it exist)
**Base-rate**: how often? (descriptive-process) how does it normally work? What are the steps?
**Relationship**: X related to Y? X occurences correlate with Y?
**Causality**: What causes it? Does X cause Y? (also comparative cauaslity) X cause Y more then Z causes Y?

## Research Strategies
Control/Realism vs Generic /Specific -> Different ways of collecting data
*Natural*: High Specific (rich context, no manipulation)
- field experiment (some controlling factor to make things comparable, experimentation, changing variables) -> **Evaluative (Causality, Base-rate)**
- field study (enter jungle, don't touch, only record) -> **Exploratory**
*Contrived*: High Control (all controlled, but lacking realism)
- lab experiment (optimize internal validity at cost of external validity, full control, not specific setting, has participants) -> **causality**
- experimental simulation (flight simulator, fully contrived environment, low generalizability, specific setting) -> **relationship and causality**
*Neutral*: High Generic (individual differences become blurry)
- judgment study (courtroom, making explicit choices of who is sampled, not representative, no specific context, not generalizable for population) -> **exploratory, classification, base-rate descriptive process**
- sample study (referendum, low precision, full population, no clarifications, reductionist, generalizable) -> **existence, base-rate, causality**
*Non-empirical:* High Realism (not immedietly based on observation)
- computer simulation (no real world data, no manipulations, low generalizability)
- formal theory (formulating relations among concepts, low realism, no empirical info gathered)

### Combining Strategies
Can use **sequential strategies** to hone in on answer, combine methods.
**Triangulation**: using multiple methods in parallel to confirm each other

## Threats to Validity
There are lots of things that can and do go wrong during research
Need to keep in mind the tools and techniques and make sure we are using them the right way. If there is something we are not or cannot control for, we need to manage it.
Observations might have something from experimental design OR population that keeps us from commenting on something we want to comment on.

Generalization needs to be valid for the population
Internal: conclusion derived for the study itself
external: generalisability of the findings
**Theoretical**
- Cause Construct : Theoretical "thing" that is the cause of something
- Effect Construct: Theoretical "thing" that is the result of the cause
**Observation**
- Treatment: Operationalisation of the cause
- Output: Operationalisation of the effect

**Construct Validity:** How valid is our operationalisation?
**External Validity**: Is the conclusion drawn between cause and effect generalizable outside scope of study? (is the sample representative of the population we are generalizing)
- wrong sample studied that isnt representative of population generalized to
- wrong environment used that isn't representative of actual industry practice
- wrong timing that changes how people answer (big event just happened, holidays etc.)
**Conclusion Validity:** Is the conclusion we drew statistically significant? (did we do the math right) 
- low stats power (significant amount of participants)
- violated assumptions, or incorrect stats tests used
- fishing/error rate -> multiple analysis needs to have adjusted error rate
- reliable measures: two measurements result in same output (affected by tools used, experiment setup)
- reliable treatment implementation: things should be standard for each participant/subject
- experiment setting is shitty (people see in window, noise, etc.)
- heterogeneity -> impact of variety of individuals might be more than impact of the treatment
**Internal Validity**: True causal relationship, and not result of some other, uncontrolled factor. (did we select correct participants, any controls or lack of controls)
- single group: no control to see how things react
	- history- the time at which a test is done on the same thing twice
	- maturation - people react differently as study progresses (bored or extra engaged)
- multiple group: control group and experiment group can be affected differently, just cause it's different groups
- social: humans are autonomous!
- 

**Design threats**: related to how the expriment is deisgned
- inadequate construct definitions before experiment is done -> things are vague, leading to confusion for participants
- confounding constructs/ levels of constructs -> the spectrum of the construct is not granular enough
- interaction of test and treatment -> subjects can change behaviour because something is being monitored
- interaction of treatments -> multiple variables being changed at the same time make it hard to see which one is having the impact
- mono-operation bias -> only a single independent variable could mean we are creating a cause/effect based on something that does not exist...
- mono-method bias -> only measuring results with a single method, without cross validating with some other methods
- restriced generalizability across constructs -> There could be some negative impact that occured that was not being measured. Need to look at all impacts and potential sacrifices (prodcutivity in coding vs maintainable code)
**Social Threats:** related to how the subjects behave during the experiment
- Hypothesis guessing -> people behaving differently based on their guess at what the research is trying to show
- Evaluation apprehension -> people perform differently when being tested
- Experimenter expectancies -> asking questions in ways to get the answer we want. Include people "farther way" from the study
- 
**EXAM:** Explain strategies, identify them in a paper. Design suitable research questions. Identify threats to validity.

# Data Collection
## Sampling 
Types + How they work
Impact

Need to be able to pick the right strategy for the right research question (e.g. depth vs breadth)
Strategies can be matched to concrete methods  
- population we want to study, there is an imperfect frame (sampling frame), and we will chose a sample from this frame
- we sample with different purposes in mind. Need to understand and pick correct strategy based on our objective
### Types 
**Non-probabilisitc**: anything without randomness
- convenience: cheap, fast, no sample frame. great for pilot
- snowball: aka referral-chain. hidden-populations often know each other. Can look at paper references (forward/ backward snowballing). Downside is sampling well connected subset
- purposive: selected based on specific characteristics, specific search query, expert sampling. Usually used to find information rich sources. Can ensure representative of some dimension. However, subjective.
- respondent driven: hardcore snow-ball, where there is some purposeful sampling of people who then recruit others, with some statistical rigor included.
**Probabilisitc**: does have randomness
- whole-frame: look at everything
- simple random: random number generator to select participants
	- representative sample is one where the sample represents the target based on some charactersitics (hard to match different dimensions)
- systematic random: systematically choose the 5th person
**Multi-stage**: 2 or more combined strategies
- stratified random: stage 1 - purposeful choice, stage 2 - random selection from sub-group
- stratified quota: stage 1 - purposeful choice, stage 2 - purposful choice
- cluster: stage 1 - random choice of subsets, stage 2 - random choice of items within subsets
	- adaptive cluster sample: interview more people based on sub-group results

**Panel Sampling**: Return to same group of people interviewed last year. probabilistic or non-probabilistic depending on the first stage.

## Interviews + Survey
How to design and run
Inteviews - deep + adapt
Surveys - scale, quantitative analysis, standardized questions

### Interviews
Interviews with Human Participants: 
- structured (specific list)
- semi-structure (list of questions that allow for devations)
	- adapt as we get answered. A survey, we cannot go back and change things.
- unstructured (just discussion on topic with free flow)
### How to Design
1) know the lingo
2) draft the questions -> based on hypothesis. Avoid double-barred (this AND this) or leading questions. No irrelevant questions. Good: explain, illustrate, reflect.
3) Review and revise the questions
4) Run a pilot (and then maybe step 3 again) -> Cannot use pilot question answers!
	- If we want to do interviews AND questions... We can have some sample interviews and surveys, and understand if people are answering them as expected before releasing the full thing
1) Do the interview (the final version is called interview guide)
Number of Participants: 25-35, but could do iterative review to see if saturation has been reached
Surveys: Based on required confidence level and interval and population size
Look at the channels from where our respondants came from for future analysis (i.e. which FB group, or reddit thread etc.)
- Anoism -> Analysis of similarities. Check if the responses of all the groups are similar enough, or if different groups have different experiences. Possible to do with quantitative data.
### Types of Questions
Can ask about facts, experiences (past stories), attitudes (is something importnat), beliefs (how often something happens), norms (what is normal)

We can ask open ended, likert scale, frequency, multiple choice questions, all of which give different info and make things easier or harder to use. Open ended requires interpretation from humans.

Demographics -> Need to be careful to find a way to appropriately anonymize data. Need consent, and uni makes use of "public task" clause.
### General Design Requirements
- Clear
- comprehensive
- acceptable (length of time, question types)
- reliable (i.e. repeatable)
	- reliability: test-retest (same answers) during pilot, inter-rater (two pilots: different people same answers), inter-coder (extent of agreement, used for open questions)
- valid (questions answers are valid for the question)
- trustworthy (openness on the process, multiple labelers for robustness)

### Mining Software Repositories
Promises and Perils: Software repositories can offer lots of info but have issues involved.
GitHub Perils:
- no notion of mainline
- history can be revised
- might not be able to track source of commits
- multiple contributors to single commit
- some projects not "real" (student, personal, study, low activity, no pull-requests, .git mirror)
- close pull requests may be merged using non git methods
- project might not be collaborative
- bots exist in projects, hard to tell them apart from humans
	- some bots are accounts, some accounts are mixed bot/human
TAKEAWAY: verify data, is it completed as expected? Make choices about sampling data.

**EXAM:*** List, describe, apply all methods. Given a research question, select appropriate data collection technique, defend its choice, and describe how to use it.

# Data Analysis

### Quantitative Analysis
Descriptive analytics and figuring out which statistics and tests we use on our data
What do we do if we have groups we want to compare?
- Wilkonsin sometimes not enough -> compared distribution of 2 categories
	- no assumptions (normality or otherwise)
	- If known distribution, use student (t) test
	- numeric or categorical
- Fischer compare between 2 categories (if distribution unknown)
	- only categorical
- T-test (student)
	- only numeric with normal distribution
- RDD
	- measuring over time in windows
- RDD models (mixed effects over time)
Statistics and Time-series analysis
- Mixed effects models (i.e. Longitudinal data analysis) 
Know the difference between: independent vairables, parameters, blocking, dependent variables
Techniques for repo data

### Qualitative Analysis
Groudned theory -> Rashina Hoda video (quite long, but it IS course material)
- process chart that we go throuhg (lit. study, iterative build etc. all on exam)
### Advanced MSR
Example: sentiment analysis. What is the process we go through?
What kind of model do we use, how do we define sentiment, what are theories used, what tools do we build, and how do we evaluate that?

**EXAM**: Given a dataset, select appropriate analysis techqnique and apply it to answer research questions. Defend the choice and describe how it's applied.


# Exam
5 closed questions (MC, 1 right answer) - 20% (18min)
1 Open Question (given context, can we come up with study design) - 40% (36min)
- **how** and **why** we will do thing. Whether we an add that a technical term
- ERB form - consider things like risks
- multi-part question with prompts
1 Open Question (reason about threats to validity) - 40% (36min)
- 4 parts (indication of categoray + description of threat)

90 minute exam

No expected to reproduce how something is calculated, but want to know that we can come up with a design strategy for an empirical study that is valide and appropriate for the setting

No QUESTIONs that check very pointed knowledge
- might be about sample, but not about exact types of sampling