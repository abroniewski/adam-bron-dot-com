**Date: 28 March 2025**

Below is a **side-by-side comparison** of the current **Shexon Demo scope** (from the 18 March 2025 update) with **key findings from all three interviews**. It indicates which **new or higher-priority features** the interviews suggest (that aren’t fully in the demo) and whether any **existing demo features** don’t come up as a high priority in the interviews.

---

## 1. Demo Scope at a Glance

1. **AI-Powered Career Guidance Chatbot** (50% complete)
    
2. **Document Upload & Processing** (60% complete)
    
3. **Resume & Application Review** (60% complete)
    
4. **Job Interview Preparation** (20% complete)
    
5. **Mentorship & Networking Matching** (mock data, 90% complete)
    
6. **Job Matching** (mock data, 90% complete)
    

---

## 2. Interview Findings – Core Needs Not Fully Addressed in Demo

1. **Advanced Technical Interview Support**
    
    - **Interviewees** (especially #3) want more than generic Q&A. They need help **articulating high-level decisions** (“Why model X?”) and explaining theoretical underpinnings.
        
    - **Demo Status**: “AI-Generated Mock Interview Questions” is started, but mostly standard. No mention of advanced, domain-specific guidance or deeper rationale coaching.
        
    - **Suggestion**: **Prioritize** adding technical “rationale explanation” modules or question sets that help users structure advanced answers.
        
2. **Structured Reflection & Feedback Mechanisms**
    
    - **Interviewees** frequently face ambiguous rejections (“You did fine, but no offer”). They lack follow-up or reflection prompts.
        
    - **Demo Status**: Some partial mention of “AI Feedback on Interview Responses,” but no robust reflection workflow (e.g., post-interview logs, recommended improvement steps).
        
    - **Suggestion**: Add a **post-interview reflection tool** or a simple “feedback request template” that helps interpret unclear rejections.
        
3. **Visa Sponsorship Strategies**
    
    - **All** users struggle with orientation-year constraints, sponsor-friendly employers, or creative bridging solutions (e.g., part-time sponsorship).
        
    - **Demo Status**: “Visa & Sponsorship Guidance” is only partially addressed. The chatbot can handle queries, but no structured “bridging route” scenario or sponsor-friendly job search is offered.
        
    - **Suggestion**: Expand the **visa guidance** feature to highlight possible fallback sponsor strategies or short-term bridging approaches.
        
4. **ATS & Keyword Optimization**
    
    - The first and second interviews emphasize that ATS auto-filtering is a **major** hurdle; synonyms, missing terms, etc.
        
    - **Demo Status**: There is a basic “Resume & Application Review,” but it’s more about general skill gaps. The doc doesn’t mention direct ATS keyword matching guidance.
        
    - **Suggestion**: Build a **keyword-check** or “synonym recommendation” feature within resume review, e.g., “Your CV lacks these key phrases from the job description.”
        
5. **Localized Job Title “Translator”**
    
    - Multiple interviewees discover roles with different local titles (“Market Researcher” vs. “Insights Manager”) only by trial and error.
        
    - **Demo Status**: Job Matching is mock data, not referencing any real local nomenclature or synonyms.
        
    - **Suggestion**: Consider adding a **mini “role translator”** that proposes local job-title variations or synonyms that might be more standard in the Netherlands.
        
6. **Networking Tools & Scripts**
    
    - All interviews highlight struggles reaching out on LinkedIn or in events. They want **templates** for cold messages, examples of “virtual coffee” approaches, or curated mentor intros.
        
    - **Demo Status**: Mentorship & Networking is near-complete but uses mock data. The scope focuses on matching to a single “fake mentor,” not in-depth messaging guidance or group events.
        
    - **Suggestion**: Introduce **networking script prompts** or a small “outreach message builder.” Possibly highlight smaller group virtual events vs. big “career fair” style.
        
7. **Emotional Support & Confidence Building**
    
    - Overarching theme: fear of rejection, ghosting, losing motivation. This was under-addressed in the original flow.
        
    - **Demo Status**: Not explicitly in the scope. The AI Chatbot does general Q&A, but no mention of confidence-boosting modules or mental health resources.
        
    - **Suggestion**: Add “Confidence Corner” or short motivational check-ins, plus tips on handling rejections, possibly with a “peer stories” resource or AMA with mentors.
        
8. **Interview “Interviewer’s Perspective”**
    
    - Interviewees want to know how hiring managers evaluate them, especially in ambiguous rejections.
        
    - **Demo Status**: The chatbot addresses general career Q&A. No specific “from the hiring manager’s lens” content.
        
    - **Suggestion**: A specialized module or an FAQ that outlines typical manager concerns, hidden evaluation criteria, and feedback styles.
        

---

## 3. Features in the Demo That Didn’t Emerge as High Priorities

1. **Meeting Scheduler for Career Services**
    
    - The demo includes an 8-hour scope item for scheduling appointments with a career advisor.
        
    - **Interview Relevance**: Students do want “hands-on” help, but none specifically requested an in-app meeting scheduler with mock timeslots. The bigger pain is lacking real mentors or 1:1 guidance, not the scheduling functionality itself.
        
    - **Consider**: This might be less urgent. The interviews stressed actual mentor or peer feedback, not so much an automated meeting arrangement.
        
2. **Events & Networking “Mocked”**
    
    - The plan calls for mock data of networking events.
        
    - **Interview Relevance**: Users want real or curated events, plus ways to get conversation scripts or smaller group intros. “Mock events” may not solve their real problem.
        
    - **Consider**: Maybe pivot resources toward generating a structured approach for actual or sample outreach in smaller events. The “fake event” approach is nice for a quick demo but not high priority based on user needs.
        
3. **AI-Suggested Certifications & Training**
    
    - The user story is only 2 hours scope. Some interviewees like the idea of skill-gap coverage but rarely singled out “certifications” as top priority. Instead, they want advanced interview or ATS guidance.
        
    - **Consider**: Keep it as a “nice to have,” but the interviews point to more pressing needs like advanced interview coaching or emotional support.
        

---

## 4. Recommended Scope Adjustments

**Prioritize / Enhance**

- **Technical & Behavioral Interview Support**: Provide deeper practice on theoretical explanations, “why this approach,” reflection prompts.
    
- **Visa Sponsorship “Playbook”**: Expand beyond generic Q&A. Possibly highlight bridging jobs or short-term sponsor strategies.
    
- **ATS & Language/Title Tools**: Offer a more robust system for checking keywords or local job-title nuances.
    
- **Emotional / Confidence Features**: Even a small “motivation & reflection” area or chatbot prompts to help handle rejection.
    
- **Networking Scripts & Tools**: Instead of just listing mentors or events, incorporate conversation templates or smaller group introductions.
    

**De-Prioritize / Limit**

- **Meeting Scheduler**: Could remain minimal for the demo. Students do want human help, but the scheduling feature itself isn’t top priority from user perspectives.
    
- **Mock Data for Large Events**: Focus on _how to approach events or mentors_, not just listing placeholders.
    
- **Extended Certification Lists**: Keep it short for now; the interviews show higher immediate impact from advanced interview and ATS guidance than from a big training directory.
    

By **shifting** some resources toward **interview-depth features** (technical rationale, reflection) and **ATS/language** enhancements—plus highlighting **emotional resilience** or **networking scripts**—you’ll align more closely with the **most urgent** user concerns from the three interviews.