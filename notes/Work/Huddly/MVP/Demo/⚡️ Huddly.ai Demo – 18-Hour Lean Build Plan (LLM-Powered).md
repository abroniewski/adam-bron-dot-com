# ðŸŽ¯ Objective
Connect the live **React + Supabase + Edge Function** front-end (HRS & questionnaire) with a backend **LLM agent** using the MExit infrastructure to create a conversational, readiness-aware coach named **Huddly**.

---

# ðŸ§© System Architecture

| Component | Project Repo | Status |
|----------|---------------|--------|
| Frontend & Questionnaire | [`huddly-succession-flow`](https://github.com/Moving-Data-Insights/huddly-succession-flow) | âœ… Live |
| Supabase DB (User & HRS data) | `huddly-succession-flow` | âœ… Live |
| Readiness Scoring (Edge Function) | `huddly-succession-flow` | âœ… Live |
| PDF + Briefing Generator | `huddly-succession-flow` | âœ… Working |
| LLM Chatbot + Tools | [`web-scraper`](https://github.com/Moving-Data-Insights/web-scraper) (`rag/app.py`) | âœ… Use as base |
| Vector DB / RAG | `web-scraper` | Optional / Deferred |

---

# ðŸ§  Execution Strategy

**Keep code where it lives.**
- Extend `web-scraper` with a new FastAPI route `/api/chat` to serve as Huddlyâ€™s brain.
- Reuse prompt and LLM logic from MExit.
- Integrate frontend to call this API after questionnaire is complete (handoff to Huddly).
- Memory, context, and scoring data flow from Supabase.

---

# â±ï¸ 18-Hour Timeline with Task Checklist

---

## â¬› Phase 1: Backend Agent Setup in `web-scraper` (Hours 0â€“2)

- [ ] In `web-scraper/rag/app.py`:
  - [ ] Add new POST endpoint: `/api/chat`
- [ ] In `web-scraper/rag/`:
  - [ ] Create `services/llm.py`
  - [ ] Add CORS middleware to allow frontend calls
- [ ] Configure `.env`:
  - `OPENAI_API_KEY`
  - `SUPABASE_URL` (optional if querying from backend)
- [ ] Deploy backend to Railway if not already

---

## â¬› Phase 2: Load Persona & Prompt System (Hours 2â€“4)

- [ ] Add `/prompts/system_prompt.md`
  - Define Huddlyâ€™s tone, background, mission
- [ ] Add `services/prompt_manager.py`
  - Centralize prompt loading
  - Expose `load_prompt(name: str) -> str`
- [ ] Copy relevant LLM logic from `briefing.py` (or edge function) into backend

---

## â¬› Phase 3: LLM Chat Route (Hours 4â€“6)

- [ ] In `/api/chat`, accept:
```json
{
  "user_id": "abc123",
  "messages": [{ "role": "user", "content": "..." }],
  "context": {
    "readiness_score": 72,
    "pillar_scores": {
      "emotional": 80,
      "financial": 60,
      "business": 70,
      "operational": 50
    },
    "primary_objective": "Sell business in 2 years"
  }
}
````

* [ ] Inject context into the prompt as structured memory
* [ ] Format messages into OpenAI chat API call
* [ ] Return `{"response": "LLM reply here"}`

---

## â¬› Phase 4: React Chat Integration (Hours 6â€“8) in `huddly-succession-flow`

* [ ] In `ChatView.jsx`:

  * [ ] Add `useChat()` hook:

    * `messages[]`, `sendMessage()`, `loading`, `error`
* [ ] On final screen of questionnaire:

  * [ ] Call `/api/chat` and send:

    * User ID (from Supabase)
    * HRS scores (already in Supabase)
    * Objective + context (1-pager output optional)
* [ ] Display Huddlyâ€™s reply in chat format
* [ ] Add text input + send button

---

## â¬› Phase 5: Hook Up Briefing Tool (Hours 8â€“10)

* [ ] Copy prompt from edge function into `/prompts/tools/briefing.md`
* [ ] Create `generate_briefing(context: dict) -> str`
* [ ] (Optional) Expose test-only `/api/tools/briefing` route

---

## â¬› Phase 6: Final Chat Polish & Error Handling (Hours 10â€“13)

* [ ] Show "Huddly is thinking..." loader in frontend
* [ ] Retry API failures once
* [ ] Scroll to latest message
* [ ] Confirm flow:

  * Questionnaire â†’ Chat â†’ GPT-4 response personalized with readiness data

---

## â¬› Phase 7: Refactor & Document (Hours 13â€“16)

* [ ] Refactor LLM tools into `services/tools/`

  * `readiness_summary.py`
  * `generate_briefing.py`
* [ ] Update README in `web-scraper/rag/`

  * How to add prompts
  * How to test `/api/chat`

---

## â¬› Phase 8: Record Demo & Final Touches (Hours 16â€“18)

* [ ] Push final code to GitHub (`main`)
* [ ] Confirm Railway backend deployment
* [ ] Test full flow: questionnaire â†’ LLM agent handoff
* [ ] Record Loom walkthrough
* [ ] Tag as `v0.1-huddly-demo-ready`

---

# âœ… What Will Be Live After 18 Hours

| Feature                                   | Status |
| ----------------------------------------- | ------ |
| `/api/chat` LLM agent                     | âœ…      |
| Supabase-fed memory (HRS, user objective) | âœ…      |
| Prompt loader + tools                     | âœ…      |
| Chat UI â†’ API flow                        | âœ…      |
| Modular backend prompt tools              | âœ…      |
| Live demo flow                            | âœ…      |

---

# ðŸ§± Deferred / Next SEPs

* Persistent memory (store user facts over time)
* Advisor matching logic
* Chat history threading (per session)
* Upload-based audit logic
* More advanced tool chaining (e.g. next steps planning)

---

# ðŸ§  Cursor IDE Configuration

```json
{
  "backend": {
    "repo": "Moving-Data-Insights/web-scraper",
    "entry": "rag/app.py",
    "add": "/api/chat (POST)",
    "validate": "Pydantic ChatRequest, ChatResponse",
    "output": "JSON with assistant_message"
  },
  "frontend": {
    "repo": "Moving-Data-Insights/huddly-succession-flow",
    "component": "ChatView.jsx",
    "connectsTo": "api/chat",
    "after": "questionnaire submission"
  },
  "prompts": {
    "directory": "/prompts/",
    "personality": "system_prompt.md",
    "tools": ["briefing.md"],
    "loader": "prompt_manager.py"
  }
}
```